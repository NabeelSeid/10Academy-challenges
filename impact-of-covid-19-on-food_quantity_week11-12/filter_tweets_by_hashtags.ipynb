{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip\n",
      "Collecting click (from nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/1f/17/6162a5fd4b2b76f2beb5933547ea5c3a27304585483edc93dedd740cf9b9/joblib-0.16.0.tar.gz\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-build-GvZ5oi/joblib/setup.py\", line 6, in <module>\n",
      "        import joblib\n",
      "      File \"joblib/__init__.py\", line 113, in <module>\n",
      "        from .memory import Memory, MemorizedResult, register_store_backend\n",
      "      File \"joblib/memory.py\", line 274\n",
      "        raise new_exc from exc\n",
      "                         ^\n",
      "    SyntaxError: invalid syntax\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-GvZ5oi/joblib/\u001b[0m\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nabil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nabil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweet-preprocessor\n",
      "  Using cached https://files.pythonhosted.org/packages/08/7e/60d1b535babb9f90e6809ad16484e8d634bc179056da7438fb8887e1524d/tweet-preprocessor-0.6.0.tar.gz\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-build-f6LmKH/tweet-preprocessor/setup.py\", line 4, in <module>\n",
      "        with open('README.rst', encoding='utf-8', errors='ignore') as f:\n",
      "    TypeError: 'errors' is an invalid keyword argument for this function\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-f6LmKH/tweet-preprocessor/\u001b[0m\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os, sys\n",
    "import string\n",
    "\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API\n",
    "from tweepy import Cursor\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import sys\n",
    "\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "!pip install tweet-preprocessor --upgrade\n",
    "import preprocessor as p\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetFilter():\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def filter_by_date(self, begin_date, end_date):\n",
    "        return self.df[(self.df.created_at > begin_date) & (self.df.created_at < end_date)]\n",
    "    \n",
    "    def filter_by_keyword(self, df, keywords):\n",
    "        return df[df.text.str.contains('|'.join(keywords), case=False)]\n",
    "    \n",
    "    def filter(self, begin_date, end_date, keywords):\n",
    "        df_filter_by_date = self.filter_by_date(begin_date, end_date)\n",
    "        return self.filter_by_keyword(df_filter_by_date, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_filter(csvs):\n",
    "    for csv in csvs:\n",
    "        tweets = pd.read_csv(csv + '_tweets.csv', parse_dates=['created_at'])\n",
    "        display(TweetFilter(tweets).filter('2020-03-01', '2020-09-01', \n",
    "                                           ['#Kenya', '#food', '#food4all','#fooddemand','#foodsecurity','#foodsupply',\n",
    "                                            '#foodinsecurity','#foodsupply','#foodsupplychain','#foodservice',\n",
    "                                            '#foodconsunption','#UhuruKenyansNeedFood','#FoodShortage']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetSearch():\n",
    "    '''\n",
    "    This is a basic class to search and download twitter data.\n",
    "    You can build up on it to extend the functionalities for more \n",
    "    sophisticated analysis\n",
    "    '''\n",
    "    def __init__(self, cols=None,auth=None):\n",
    "        #\n",
    "        if not cols is None:\n",
    "            self.cols = cols\n",
    "        else:\n",
    "            self.cols = ['id', 'created_at', 'source', 'original_text','clean_text', \n",
    "                    'sentiment','polarity','subjectivity', 'lang',\n",
    "                    'favorite_count', 'retweet_count','likes' 'original_author',   \n",
    "                    'possibly_sensitive', 'hashtags',\n",
    "                    'user_mentions', 'place', 'place_coord_boundaries']\n",
    "            \n",
    "        if auth is None:\n",
    "            \n",
    "            #Variables that contains the user credentials to access Twitter API \n",
    "            consumer_key = os.environ.get('consumer_key')\n",
    "            consumer_secret = os.environ.get('consumer_secret')\n",
    "            access_token = os.environ.get('access_token')\n",
    "            access_token_secret = os.environ.get('access_token_secret')\n",
    "            \n",
    "\n",
    "\n",
    "            #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "            auth = OAuthHandler('3v4UhqgqIlEDtdgrkcg42tJLi', 'kyn9yxi40ROVDILU9bZrl3QezVgB3AqZBdYYoHfabiIrbLA7XJ')\n",
    "            auth.set_access_token('1277506900628779008-O60A3EdAkacJ0Jaa3x0JXOcHjXbzt4', 'vmjZZcWNVCli6PfykbxZqsXeInAs41Lkq5KTqP28j0luD')\n",
    "            \n",
    "\n",
    "        #            \n",
    "        self.auth = auth\n",
    "        self.api = tweepy.API(auth, wait_on_rate_limit=True) \n",
    "        self.filtered_tweet = ''\n",
    "            \n",
    "\n",
    "    def clean_tweets(self, twitter_text):\n",
    "\n",
    "        #use pre processor\n",
    "        tweet = p.clean(twitter_text)\n",
    "\n",
    "         #HappyEmoticons\n",
    "        emoticons_happy = set([\n",
    "            ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "            ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "            '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "            'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "            '<3'\n",
    "            ])\n",
    "\n",
    "        # Sad Emoticons\n",
    "        emoticons_sad = set([\n",
    "            ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "            ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "            ':c', ':{', '>:\\\\', ';('\n",
    "            ])\n",
    "\n",
    "        #Emoji patterns\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                 u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                 u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                 u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                 u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                 u\"\\U00002702-\\U000027B0\"\n",
    "                 u\"\\U000024C2-\\U0001F251\"\n",
    "                 \"]+\", flags=re.UNICODE)\n",
    "\n",
    "        #combine sad and happy emoticons\n",
    "        emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = nltk.word_tokenize(tweet)\n",
    "        #after tweepy preprocessing the colon symbol left remain after      \n",
    "        #removing mentions\n",
    "        tweet = re.sub(r':', '', tweet)\n",
    "        tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "\n",
    "        #replace consecutive non-ASCII characters with a space\n",
    "        tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "\n",
    "        #remove emojis from tweet\n",
    "        tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "        #filter using NLTK library append it to a string\n",
    "        filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "        #looping through conditions\n",
    "        filtered_tweet = []    \n",
    "        for w in word_tokens:\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "            if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "                filtered_tweet.append(w)\n",
    "\n",
    "        return ' '.join(filtered_tweet)            \n",
    "    \n",
    "    def get_user_tweets(self, screen_name, begin_date, end_date, keywords):\n",
    "        '''\n",
    "        Search and return tweets of a user \n",
    "        '''\n",
    "#         auth = tweepy.OAuthHandler('RPFdqfW02zjJBUz4own9u1bbq', 'FeGaoXU3MkKLpEtJW8OYoOlA0yjGVh4Pn3aqkcwWDwuHC9eUsE')\n",
    "#         auth.set_access_token('1277497429856866305-vcP7taySdn9c8Gl2x13oaOJzWAoDjt', 'N46WpNcZgn6me2DTto0LsXtjPc8aQUu2WrKnPKp27rqZm')\n",
    "#         api = tweepy.API(auth)\n",
    "\n",
    "        #initialize a list to hold all the tweepy Tweets\n",
    "        alltweets = []  \n",
    "\n",
    "        #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "        new_tweets = self.api.user_timeline(screen_name = screen_name, count=200)\n",
    "\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #save the id of the oldest tweet minus one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        #keep grabbing tweets until there are no tweets left to grab. \n",
    "        # Limit set to around 3k tweets, can be edited to preferred number.\n",
    "        while len(new_tweets) > 0:\n",
    "            print(\"getting tweets before %s\" % (oldest))\n",
    "\n",
    "            #all subsiquent requests use the max_id arg to prevent duplicates\n",
    "            new_tweets = self.api.user_timeline(screen_name = screen_name,count=200, max_id=oldest)\n",
    "\n",
    "            #save most recent tweets\n",
    "            alltweets.extend(new_tweets)\n",
    "\n",
    "            #update the id of the oldest tweet less one\n",
    "            oldest = alltweets[-1].id - 1\n",
    "\n",
    "            print(\"...%s tweets downloaded so far\" % (len(alltweets)))   \n",
    "        print('finish user')\n",
    "        #transform the tweets into a 2D array that will populate the csv \n",
    "        outtweets = [[tweet.id_str,tweet.created_at,screen_name,tweet.retweet_count,tweet.favorite_count, \n",
    "                      self.clean_tweets(tweet.text)] for tweet in alltweets]\n",
    "        \n",
    "        # .encode(\"utf-8\")\n",
    "        \n",
    "        tweets = pd.DataFrame(outtweets, columns=[\"id\",\"created_at\",\"screen_name\",\"retweet_count\",\"favorite_count\",\"text\"])\n",
    "        return TweetFilter(tweets).filter(begin_date, end_date, keywords)\n",
    "#         #write the csv  \n",
    "#         with open('%s_tweets.csv' % screen_name, 'w') as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerow([\"id\",\"created_at\",\"retweet_count\",\"favorite_count\",\"text\"])\n",
    "#             writer.writerows(outtweets)\n",
    "        \n",
    "    def get_usernames(self, hashtags):\n",
    "        '''\n",
    "        Search for latest tweets that contains a hashtag and return\n",
    "        tweet holders' username\n",
    "        '''\n",
    "        names = []\n",
    "        for hashtag in hashtags:\n",
    "            tweets = tweepy.Cursor(self.api.search, q = hashtag ,geocode=\"0.0236,37.9062,1000km\").items(1000000)\n",
    "            users = []\n",
    "            for status in tweets:\n",
    "                name = status.user.screen_name\n",
    "                t=status.text\n",
    "                users.append(name)\n",
    "            names.append(users)\n",
    "        return [y for x in names for y in x]\n",
    "    \n",
    "    def a(self, begin_date, end_date, keywords, filter_keywords):\n",
    "        \n",
    "        end_date = '2020-10-01' if(not end_date) else end_date\n",
    "        begin_date = '2020-01-01' if (not begin_date) else begin_date\n",
    "        \n",
    "        hashtags=['#food4all','#fooddemand','#foodsecurity','#foodsupply',\n",
    "                  '#foodinsecurity','#foodsupply','#foodsupplychain',\n",
    "                  '#foodservice','#foodconsunption','#UhuruKenyansNeedFood','#FoodShortage']\n",
    "        print('Start')\n",
    "        user_names = self.get_usernames(keywords)\n",
    "        print('username end')\n",
    "        \n",
    "        tweet_output = pd.DataFrame(columns=[\"id\",\"created_at\",\"screen_name\",\"retweet_count\",\"favorite_count\",\"text\"])\n",
    "        \n",
    "        for username in list(set(user_names))[0:5]:\n",
    "            print(username)\n",
    "            tweet_user = self.get_user_tweets(username, begin_date, end_date, filter_keywords)\n",
    "            tweet_output = pd.concat([tweet_output, tweet_user])\n",
    "        \n",
    "        return tweet_output\n",
    "        \n",
    "    def get_tweets(self, keyword, csvfile=None):\n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame(columns=self.cols)\n",
    "        \n",
    "        if not csvfile is None:\n",
    "            #If the file exists, then read the existing data from the CSV file.\n",
    "            if os.path.exists(csvfile):\n",
    "                df = pd.read_csv(csvfile, header=0)\n",
    "            \n",
    "\n",
    "        #page attribute in tweepy.cursor and iteration\n",
    "        for page in tweepy.Cursor(self.api.search, q=keyword,count=500, include_rts=False).pages():\n",
    "\n",
    "            # the you receive from the Twitter API is in a JSON format and has quite an amount of information attached\n",
    "            for status in page:\n",
    "                \n",
    "                new_entry = []\n",
    "                status = status._json\n",
    "                \n",
    "                #filter by language\n",
    "                if status['lang'] != 'en':\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                #if this tweet is a retweet update retweet count\n",
    "                if status['created_at'] in df['created_at'].values:\n",
    "                    i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
    "                    #\n",
    "                    cond1 = status['favorite_count'] != df.at[i, 'favorite_count']\n",
    "                    cond2 = status['retweet_count'] != df.at[i, 'retweet_count']\n",
    "                    if cond1 or cond2:\n",
    "                        df.at[i, 'favorite_count'] = status['favorite_count']\n",
    "                        df.at[i, 'retweet_count'] = status['retweet_count']\n",
    "                    continue\n",
    "\n",
    "                #calculate sentiment\n",
    "                filtered_tweet = self.clean_tweets(status['text'])\n",
    "                blob = TextBlob(filtered_tweet)\n",
    "                Sentiment = blob.sentiment     \n",
    "                polarity = Sentiment.polarity\n",
    "                subjectivity = Sentiment.subjectivity\n",
    "\n",
    "                new_entry += [status['id'], status['created_at'],\n",
    "                              status['source'], status['text'], filtered_tweet, \n",
    "                              Sentiment,polarity,subjectivity, status['lang'],\n",
    "                              status['favorite_count'], status['retweet_count']]\n",
    "\n",
    "                new_entry.append(status['user']['screen_name'])\n",
    "\n",
    "                try:\n",
    "                    is_sensitive = status['possibly_sensitive']\n",
    "                except KeyError:\n",
    "                    is_sensitive = None\n",
    "\n",
    "                new_entry.append(is_sensitive)\n",
    "\n",
    "                hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
    "                new_entry.append(hashtags) #append the hashtags\n",
    "\n",
    "                #\n",
    "                mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
    "                new_entry.append(mentions) #append the user mentions\n",
    "\n",
    "                try:\n",
    "                    xyz = status['place']['bounding_box']['coordinates']\n",
    "                    coordinates = [coord for loc in xyz for coord in loc]\n",
    "                except TypeError:\n",
    "                    coordinates = None\n",
    "                #\n",
    "                new_entry.append(coordinates)\n",
    "\n",
    "                try:\n",
    "                    location = status['user']['location']\n",
    "                except TypeError:\n",
    "                    location = ''\n",
    "                #\n",
    "                new_entry.append(location)\n",
    "\n",
    "                #now append a row to the dataframe\n",
    "                single_tweet_df = pd.DataFrame([new_entry], columns=self.cols)\n",
    "                df = df.append(single_tweet_df, ignore_index=True)\n",
    "\n",
    "        if not csvfile is None:\n",
    "            #save it to file\n",
    "            df.to_csv(csvfile, columns=self.cols, index=False, encoding=\"utf-8\")\n",
    "            \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['#food4all','#fooddemand','#foodsecurity','#foodsupply',\n",
    "                  '#foodinsecurity','#foodsupply','#foodsupplychain',\n",
    "                  '#foodservice','#foodconsunption','#UhuruKenyansNeedFood','#FoodShortage']\n",
    "\n",
    "filter_keywords = ['food','food4all','fooddemand','foodsecurity',\n",
    "                'foodsupply', 'food supply','food insecurity','foodinsecurity',\n",
    "                'foodsupplychain', 'food supply chain','food service','foodservice',\n",
    "                'foodconsumption', 'food consumption' 'UhuruKenyansNeedFood',\n",
    "                'Uhuru Kenyans Need Food','FoodShortage', 'Food Shortage']\n",
    "\n",
    "tweet_search = TweetSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "username end\n",
      "MwangiWilson28\n",
      "getting tweets before 1301562375162691585\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1292747971289915392\n",
      "...593 tweets downloaded so far\n",
      "getting tweets before 1282241298511142912\n",
      "...790 tweets downloaded so far\n",
      "getting tweets before 1269213474003058687\n",
      "...811 tweets downloaded so far\n",
      "getting tweets before 1168857673959448576\n",
      "...811 tweets downloaded so far\n",
      "finish user\n",
      "NjorogeAnnie\n",
      "getting tweets before 1130841589834641407\n",
      "...335 tweets downloaded so far\n",
      "getting tweets before 180282809024397311\n",
      "...335 tweets downloaded so far\n",
      "finish user\n",
      "GreenTreasuresF\n",
      "getting tweets before 1265208424649932799\n",
      "...50 tweets downloaded so far\n",
      "finish user\n",
      "Ukoofarmproduc1\n",
      "getting tweets before 1302558480663031807\n",
      "...38 tweets downloaded so far\n",
      "finish user\n",
      "_PABRA\n",
      "getting tweets before 1225729704789004287\n",
      "...397 tweets downloaded so far\n",
      "getting tweets before 1150662803512004607\n",
      "...597 tweets downloaded so far\n",
      "getting tweets before 1064447440760250368\n",
      "...797 tweets downloaded so far\n",
      "getting tweets before 976031184089174016\n",
      "...997 tweets downloaded so far\n",
      "getting tweets before 920944997880475647\n",
      "...1197 tweets downloaded so far\n",
      "getting tweets before 796647402061135871\n",
      "...1397 tweets downloaded so far\n",
      "getting tweets before 750373729562136575\n",
      "...1597 tweets downloaded so far\n",
      "getting tweets before 717605823338455039\n",
      "...1797 tweets downloaded so far\n",
      "getting tweets before 694875041658437633\n",
      "...1996 tweets downloaded so far\n",
      "getting tweets before 662601763825033216\n",
      "...2196 tweets downloaded so far\n",
      "getting tweets before 647212817754423295\n",
      "...2283 tweets downloaded so far\n",
      "getting tweets before 451243158161358847\n",
      "...2283 tweets downloaded so far\n",
      "finish user\n"
     ]
    }
   ],
   "source": [
    "first_case = '2020-03-13'\n",
    "\n",
    "lockdown_started = '2020-04-06'\n",
    "lockdown_ended = '2020-07-06'\n",
    "\n",
    "final_tweet = tweet_search.a(begin_date=False, end_date=first_case, keywords=keywords, filter_keywords=filter_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1214509467376783360</td>\n",
       "      <td>2020-01-07 11:29:56</td>\n",
       "      <td>NjorogeAnnie</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yeah 's fanaticism dilutes real threats like f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1214505208354615298</td>\n",
       "      <td>2020-01-07 11:13:00</td>\n",
       "      <td>NjorogeAnnie</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Sever food insecurity horizon But I would n't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1236599544512761856</td>\n",
       "      <td>2020-03-08 10:28:00</td>\n",
       "      <td>_PABRA</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Zurich EPFL invites applications Future Food F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1226775790706208768</td>\n",
       "      <td>2020-02-10 07:51:55</td>\n",
       "      <td>_PABRA</td>\n",
       "      <td>15</td>\n",
       "      <td>33</td>\n",
       "      <td>It great honor recognition bean research excel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id          created_at   screen_name retweet_count  \\\n",
       "62   1214509467376783360 2020-01-07 11:29:56  NjorogeAnnie             0   \n",
       "63   1214505208354615298 2020-01-07 11:13:00  NjorogeAnnie             1   \n",
       "159  1236599544512761856 2020-03-08 10:28:00        _PABRA             2   \n",
       "190  1226775790706208768 2020-02-10 07:51:55        _PABRA            15   \n",
       "\n",
       "    favorite_count                                               text  \n",
       "62               1  Yeah 's fanaticism dilutes real threats like f...  \n",
       "63               4  Sever food insecurity horizon But I would n't ...  \n",
       "159              0  Zurich EPFL invites applications Future Food F...  \n",
       "190             33  It great honor recognition bean research excel...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
